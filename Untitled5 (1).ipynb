{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aboda\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "[[[0.43962264 0.56037736]\n",
      "  [0.39169473 0.60830527]\n",
      "  [0.40180587 0.59819413]\n",
      "  [0.42316514 0.57683486]\n",
      "  [0.42151482 0.57848518]\n",
      "  [0.4332211  0.5667789 ]\n",
      "  [0.43425414 0.56574586]\n",
      "  [0.52708804 0.47291196]\n",
      "  [0.48510131 0.51489869]\n",
      "  [0.45350052 0.54649948]\n",
      "  [0.46137787 0.53862213]\n",
      "  [0.4628866  0.5371134 ]\n",
      "  [0.46348884 0.53651116]\n",
      "  [0.46292585 0.53707415]\n",
      "  [0.36254107 0.63745893]\n",
      "  [0.38895152 0.61104848]\n",
      "  [0.38895152 0.61104848]\n",
      "  [0.38116592 0.61883408]\n",
      "  [0.39029768 0.60970232]\n",
      "  [0.38943894 0.61056106]\n",
      "  [0.36965377 0.63034623]\n",
      "  [0.40218471 0.59781529]\n",
      "  [0.40865385 0.59134615]]]\n",
      "INFO:tensorflow:Restoring parameters from ./gesture_model\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, [None, 23, 2])\n",
    "n_input=2\n",
    "n_hidden=32\n",
    "n_classes=2\n",
    "\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "pred = LSTM_RNN(x, weights, biases)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "# Environment:\n",
    "# OS    : Mac OS EL Capitan\n",
    "# python: 3.5\n",
    "# opencv: 2.4.13\n",
    "\n",
    "# parameters\n",
    "cap_region_x_begin=0  # start point/total width\n",
    "cap_region_y_end=1  # start point/total width\n",
    "threshold = 60  #  BINARY threshold\n",
    "blurValue = 41  # GaussianBlur parameter\n",
    "bgSubThreshold = 50\n",
    "\n",
    "# variables\n",
    "isBgCaptured = 0   # bool, whether the background captured\n",
    "triggerSwitch = False  # if true, keyborad simulator works\n",
    "\n",
    "def printThreshold(thr):\n",
    "    print(\"! Changed threshold to \"+str(thr))\n",
    "\n",
    "\n",
    "def removeBG(frame):\n",
    "    fgmask = bgModel.apply(frame)\n",
    "    # kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    # res = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    fgmask = cv2.erode(fgmask, kernel, iterations=1)\n",
    "    res = cv2.bitwise_and(frame, frame, mask=fgmask)\n",
    "    return res\n",
    "\n",
    "\n",
    "def calculateFingers(res,drawing):  # -> finished bool, cnt: finger count\n",
    "    #  convexity defect\n",
    "    hull = cv2.convexHull(res, returnPoints=False)\n",
    "    if len(hull) > 3:\n",
    "        defects = cv2.convexityDefects(res, hull)\n",
    "        if type(defects) != type(None):  # avoid crashing.   (BUG not found)\n",
    "\n",
    "            cnt = 0\n",
    "            for i in range(defects.shape[0]):  # calculate the angle\n",
    "                s, e, f, d = defects[i][0]\n",
    "                start = tuple(res[s][0])\n",
    "                end = tuple(res[e][0])\n",
    "                far = tuple(res[f][0])\n",
    "                a = math.sqrt((end[0] - start[0]) ** 2 + (end[1] - start[1]) ** 2)\n",
    "                b = math.sqrt((far[0] - start[0]) ** 2 + (far[1] - start[1]) ** 2)\n",
    "                c = math.sqrt((end[0] - far[0]) ** 2 + (end[1] - far[1]) ** 2)\n",
    "                angle = math.acos((b ** 2 + c ** 2 - a ** 2) / (2 * b * c))  # cosine theorem\n",
    "                if angle <= math.pi / 2:  # angle less than 90 degree, treat as fingers\n",
    "                    cnt += 1\n",
    "                    cv2.circle(drawing, far, 8, [211, 84, 0], -1)\n",
    "            return True, cnt\n",
    "    return False, 0\n",
    "\n",
    "\n",
    "# Camera\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "camera.set(10,200)\n",
    "cv2.namedWindow('trackbar')\n",
    "cv2.createTrackbar('trh1', 'trackbar', threshold, 100, printThreshold)\n",
    "j=0\n",
    "ret, frame = camera.read()\n",
    "bgModel = cv2.createBackgroundSubtractorMOG2(0, bgSubThreshold)\n",
    "isBgCaptured = 1\n",
    "l=[]\n",
    "while len(l)<23:\n",
    "    print(j)\n",
    "    ret, frame = camera.read()\n",
    "    if frame.size:\n",
    "        \n",
    "        frame = cv2.resize(frame, (480, 640)) \n",
    "    \n",
    "        threshold = cv2.getTrackbarPos('trh1', 'trackbar')\n",
    "        frame = cv2.bilateralFilter(frame, 5, 50, 100)  # smoothing filter\n",
    "        frame = cv2.flip(frame, 1)  # flip the frame horizontally\n",
    "        cv2.rectangle(frame, (int(cap_region_x_begin * frame.shape[1]), 0),\n",
    "                     (frame.shape[1], int(cap_region_y_end * frame.shape[0])), (255, 0, 0), 2)\n",
    "       # cv2.imshow('original', frame)\n",
    "        \n",
    "    #  Main operation\n",
    "        if isBgCaptured == 1:  # this part wont run until background captured\n",
    "            img = removeBG(frame)\n",
    "            img = img[0:int(cap_region_y_end * frame.shape[0]),\n",
    "                        int(cap_region_x_begin * frame.shape[1]):frame.shape[1]]  # clip the ROI\n",
    "           # cv2.imshow('mask', img)\n",
    "\n",
    "        # convert the image into binary image\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            blur = cv2.GaussianBlur(gray, (blurValue, blurValue), 0)\n",
    "            #cv2.imshow('blur', blur)\n",
    "            ret, thresh = cv2.threshold(blur, threshold, 255, cv2.THRESH_BINARY)\n",
    "            #cv2.imshow('ori', thresh)\n",
    "\n",
    "\n",
    "        # get the coutours\n",
    "            thresh1 = copy.deepcopy(thresh)\n",
    "            im,contours, hierarchy = cv2.findContours(thresh1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            length = len(contours)\n",
    "            maxArea = -1\n",
    "            if length > 0:\n",
    "                for i in range(length):  # find the biggest contour (according to area)\n",
    "                    temp = contours[i]\n",
    "                    area = cv2.contourArea(temp)\n",
    "                    if area > maxArea:\n",
    "                        maxArea = area\n",
    "                        ci = i\n",
    "\n",
    "                res = contours[ci]\n",
    "                hull = cv2.convexHull(res)\n",
    "                M = cv2.moments(res)\n",
    "                cx=0\n",
    "                cy=0\n",
    "            \n",
    "                if M['m00']!=0  :\n",
    "                    cx = int(M['m10']/M['m00'])\n",
    "                    cy = int(M['m01']/M['m00'])\n",
    "                    c=cx+cy\n",
    "                    cx=cx/c\n",
    "                    cy=cy/c\n",
    "                    l.append([cx,cy])\n",
    "                    j=j+1\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                with open('E:\\\\s.pkl', 'wb') as fp:\n",
    "                \n",
    "                    pickle.dump(l, fp)\n",
    "                \n",
    "            \n",
    "                drawing = np.zeros(img.shape, np.uint8)\n",
    "                cv2.drawContours(drawing, [res], 0, (0, 255, 0), 2)\n",
    "                cv2.drawContours(drawing, [hull], 0, (0, 0, 255), 3)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "                isFinishCal,cnt = calculateFingers(res,drawing)\n",
    "                if triggerSwitch is True:\n",
    "                    if isFinishCal is True and cnt <= 2:\n",
    "                        print (cnt)\n",
    "                        app('System Events').keystroke(' ')  # simulate pressing blank space\n",
    "\n",
    "            c#v2.imshow('output', drawing)\n",
    "        \n",
    "    # Keyboard OP\n",
    "    \n",
    "xx=np.array([l])\n",
    "\n",
    "print(xx)\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "  \n",
    "    saver.restore(sess, \"./gesture_model\")\n",
    "    \n",
    "\n",
    "    p = sess.run(\n",
    "        [pred],\n",
    "        feed_dict={\n",
    "            x: xx\n",
    "           \n",
    "        }\n",
    "    )\n",
    "\n",
    "    \n",
    "    s=np.argmax(p)\n",
    "    if s==0:\n",
    "        im=cv2.imread (\"2.5.jpg\",1)\n",
    "        cv2.imshow('image',im)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        im=cv2.imread (\"2.9.jpg\",1)\n",
    "        cv2.imshow('image',im)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4444444444444444, 0.5555555555555556], [0.39352428393524286, 0.6064757160647571], [0.37340425531914895, 0.6265957446808511], [0.3832077502691066, 0.6167922497308934], [0.3641851106639839, 0.635814889336016], [0.4088200238379023, 0.5911799761620977], [0.4227441285537701, 0.5772558714462299], [0.473604826546003, 0.526395173453997], [0.41755634638196915, 0.5824436536180309], [0.4230769230769231, 0.5769230769230769], [0.4818577648766328, 0.5181422351233672], [0.3790322580645161, 0.6209677419354839], [0.35434782608695653, 0.6456521739130435], [0.33543638275499477, 0.6645636172450052], [0.41149943630214203, 0.588500563697858], [0.33579881656804733, 0.6642011834319527], [0.3352941176470588, 0.6647058823529411], [0.33138686131386863, 0.6686131386861314], [0.32230215827338127, 0.6776978417266187], [0.21159420289855072, 0.7884057971014493], [0.20970042796005706, 0.7902995720399429], [0.20679886685552407, 0.7932011331444759], [0.20538243626062322, 0.7946175637393768]]\n"
     ]
    }
   ],
   "source": [
    "with open ('E:\\\\s.pkl', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)\n",
    "print(itemlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aboda\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n",
      "46\n",
      "20\n",
      "2\n",
      "[[0.28473803 0.71526194]\n",
      " [0.28392246 0.71607757]\n",
      " [0.42607003 0.57392997]\n",
      " [0.29893237 0.7010676 ]\n",
      " [0.3067633  0.6932367 ]\n",
      " [0.31565967 0.6843403 ]\n",
      " [0.3164721  0.6835279 ]\n",
      " [0.33425796 0.66574204]\n",
      " [0.34582132 0.6541787 ]\n",
      " [0.36875    0.63125   ]\n",
      " [0.4088586  0.5911414 ]\n",
      " [0.392555   0.607445  ]\n",
      " [0.42344046 0.57655954]\n",
      " [0.417603   0.582397  ]\n",
      " [0.41491395 0.58508605]\n",
      " [0.35493827 0.64506173]\n",
      " [0.34359807 0.65640193]\n",
      " [0.33280757 0.6671924 ]\n",
      " [0.35433072 0.6456693 ]\n",
      " [0.34477612 0.6552239 ]\n",
      " [0.33001423 0.6699858 ]\n",
      " [0.32085562 0.6791444 ]\n",
      " [0.3109137  0.6890863 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "LABELS = [    \n",
    "    \"JUMPING\",\n",
    "    \"JUMPING_JACKS\",\n",
    "    \"BOXING\",\n",
    "    \"WAVING_2HANDS\",\n",
    "    \"WAVING_1HAND\",\n",
    "    \"CLAPPING_HANDS\"\n",
    "\n",
    "] \n",
    "X_train_path = \"x.txt\"\n",
    "X_test_path = \"x_t.txt\"\n",
    "\n",
    "y_train_path = \"y.txt\"\n",
    "y_test_path = \"y_t.txt\"\n",
    "n_steps = 23\n",
    "\n",
    "def load_X(X_path):\n",
    "    file = open(X_path, 'r')\n",
    "    X_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.split(',') for row in file\n",
    "        ]], \n",
    "        dtype=np.float32\n",
    "    )\n",
    "    file.close()\n",
    "    print(len(X_))\n",
    "    blocks = int(len(X_) / n_steps)\n",
    "    \n",
    "    X_ = np.array(np.split(X_,blocks))\n",
    "\n",
    "    return X_ \n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    X_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row for row in file\n",
    "        ]], \n",
    "        dtype=np.float32\n",
    "    )\n",
    "    file.close()\n",
    "    print(len(X_))\n",
    "    blocks = int(len(X_) / 1)\n",
    "    \n",
    "    X_ = np.array(np.split(X_,blocks))\n",
    "\n",
    "    return X_ \n",
    "X_train = load_X(X_train_path)\n",
    "X_test = load_X(X_test_path)\n",
    "#print X_test\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "\n",
    "y_test = load_y(y_test_path)\n",
    "print(X_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(20, 23, 2) (2, 1) 0.5 0.17427889\n",
      "\n",
      "The dataset has not been preprocessed, is not normalised etc\n"
     ]
    }
   ],
   "source": [
    "training_data_count = len(X_train)  # 4519 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)\n",
    "n_input = len(X_train[0][0])  # num input parameters per timestep\n",
    "\n",
    "n_hidden = 32 # Hidden layer num of features\n",
    "n_classes = 2 \n",
    "\n",
    "learning_rate = 0.0025\n",
    "lambda_loss_amount = 0.0015\n",
    "\n",
    "training_iters = training_data_count *600  # Loop 600 times on the dataset\n",
    "batch_size = 20\n",
    "display_iter = 100  # To show test set accuracy during training\n",
    "\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_train.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "print(\"\\nThe dataset has not been preprocessed, is not normalised etc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # model architecture based on \"guillaume-chevalier\" and \"aymericdamien\" under the MIT license.\n",
    "\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    _X = tf.reshape(_X, [-1, n_input])   \n",
    "    # Rectifies Linear Unit activation function used\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0) \n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # A single output is produced, in style of \"many to one\" classifier, refer to http://karpathy.github.io/2015/05/21/rnn-effectiveness/ for details\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y_):\n",
    "    # One hot encoding of the network outputs\n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e68446dca84b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Graph input/output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Graph input/output\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "# Loss, optimizer and evaluation\n",
    "l2 = lambda_loss_amount * sum(\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    ") # L2 loss prevents this overkill neural network to overfit the data\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #20:   Batch Loss = 1.100581, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5284746885299683, Accuracy = 0.5\n",
      "Training iter #100:   Batch Loss = 1.099326, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.115683913230896, Accuracy = 0.5\n",
      "Training iter #200:   Batch Loss = 0.990461, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0203384160995483, Accuracy = 0.5\n",
      "Training iter #300:   Batch Loss = 0.914832, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9519663453102112, Accuracy = 0.5\n",
      "Training iter #400:   Batch Loss = 0.883485, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9045260548591614, Accuracy = 1.0\n",
      "Training iter #500:   Batch Loss = 0.849847, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8658789396286011, Accuracy = 0.5\n",
      "Training iter #600:   Batch Loss = 0.799613, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7796359658241272, Accuracy = 1.0\n",
      "Training iter #700:   Batch Loss = 0.753272, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6760254502296448, Accuracy = 1.0\n",
      "Training iter #800:   Batch Loss = 0.709284, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6330463886260986, Accuracy = 1.0\n",
      "Training iter #900:   Batch Loss = 0.664236, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.5676466226577759, Accuracy = 1.0\n",
      "Training iter #1000:   Batch Loss = 0.619876, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.46119219064712524, Accuracy = 1.0\n",
      "Training iter #1100:   Batch Loss = 0.558674, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2984354794025421, Accuracy = 1.0\n",
      "Training iter #1200:   Batch Loss = 0.561140, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2736019194126129, Accuracy = 1.0\n",
      "Training iter #1300:   Batch Loss = 0.460699, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.26109015941619873, Accuracy = 1.0\n",
      "Training iter #1400:   Batch Loss = 0.425508, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.25547513365745544, Accuracy = 1.0\n",
      "Training iter #1500:   Batch Loss = 0.452742, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.24906177818775177, Accuracy = 1.0\n",
      "Training iter #1600:   Batch Loss = 0.365853, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2500489056110382, Accuracy = 1.0\n",
      "Training iter #1700:   Batch Loss = 0.373675, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.25366562604904175, Accuracy = 1.0\n",
      "Training iter #1800:   Batch Loss = 0.371782, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.24549321830272675, Accuracy = 1.0\n",
      "Training iter #1900:   Batch Loss = 0.303941, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.24214857816696167, Accuracy = 1.0\n",
      "Training iter #2000:   Batch Loss = 0.281324, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.24031129479408264, Accuracy = 1.0\n",
      "Training iter #2100:   Batch Loss = 0.311923, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2414727807044983, Accuracy = 1.0\n",
      "Training iter #2200:   Batch Loss = 0.398088, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.353360652923584, Accuracy = 1.0\n",
      "Training iter #2300:   Batch Loss = 0.329599, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2375839352607727, Accuracy = 1.0\n",
      "Training iter #2400:   Batch Loss = 0.261838, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2369556576013565, Accuracy = 1.0\n",
      "Training iter #2500:   Batch Loss = 0.258152, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.23448148369789124, Accuracy = 1.0\n",
      "Training iter #2600:   Batch Loss = 0.251125, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.23310278356075287, Accuracy = 1.0\n",
      "Training iter #2700:   Batch Loss = 0.243114, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.23191648721694946, Accuracy = 1.0\n",
      "Training iter #2800:   Batch Loss = 0.238132, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.23089376091957092, Accuracy = 1.0\n",
      "Training iter #2900:   Batch Loss = 0.235493, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.22983013093471527, Accuracy = 1.0\n",
      "Training iter #3000:   Batch Loss = 0.232739, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.22860939800739288, Accuracy = 1.0\n",
      "Training iter #3100:   Batch Loss = 0.230799, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.22729429602622986, Accuracy = 1.0\n",
      "Training iter #3200:   Batch Loss = 0.229093, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2258545309305191, Accuracy = 1.0\n",
      "Training iter #3300:   Batch Loss = 0.227245, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.22432664036750793, Accuracy = 1.0\n",
      "Training iter #3400:   Batch Loss = 0.225586, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2227800488471985, Accuracy = 1.0\n",
      "Training iter #3500:   Batch Loss = 0.223877, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.22123965620994568, Accuracy = 1.0\n",
      "Training iter #3600:   Batch Loss = 0.222247, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.21970145404338837, Accuracy = 1.0\n",
      "Training iter #3700:   Batch Loss = 0.220628, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.21817341446876526, Accuracy = 1.0\n",
      "Training iter #3800:   Batch Loss = 0.219044, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.21666742861270905, Accuracy = 1.0\n",
      "Training iter #3900:   Batch Loss = 0.217485, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.21518100798130035, Accuracy = 1.0\n",
      "Training iter #4000:   Batch Loss = 0.215948, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.21370990574359894, Accuracy = 1.0\n",
      "Training iter #4100:   Batch Loss = 0.214435, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.21225838363170624, Accuracy = 1.0\n",
      "Training iter #4200:   Batch Loss = 0.212942, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.21082837879657745, Accuracy = 1.0\n",
      "Training iter #4300:   Batch Loss = 0.211471, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.20941464602947235, Accuracy = 1.0\n",
      "Training iter #4400:   Batch Loss = 0.210019, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.20801614224910736, Accuracy = 1.0\n",
      "Training iter #4500:   Batch Loss = 0.208586, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.20663470029830933, Accuracy = 1.0\n",
      "Training iter #4600:   Batch Loss = 0.207171, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2052694410085678, Accuracy = 1.0\n",
      "Training iter #4700:   Batch Loss = 0.205773, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.203918918967247, Accuracy = 1.0\n",
      "Training iter #4800:   Batch Loss = 0.204394, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2025846391916275, Accuracy = 1.0\n",
      "Training iter #4900:   Batch Loss = 0.203031, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.2012666016817093, Accuracy = 1.0\n",
      "Training iter #5000:   Batch Loss = 0.201685, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.19996370375156403, Accuracy = 1.0\n",
      "Training iter #5100:   Batch Loss = 0.200354, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1986760050058365, Accuracy = 1.0\n",
      "Training iter #5200:   Batch Loss = 0.199040, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.19740350544452667, Accuracy = 1.0\n",
      "Training iter #5300:   Batch Loss = 0.197741, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.19614489376544952, Accuracy = 1.0\n",
      "Training iter #5400:   Batch Loss = 0.196457, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.194900244474411, Accuracy = 1.0\n",
      "Training iter #5500:   Batch Loss = 0.195188, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.19366949796676636, Accuracy = 1.0\n",
      "Training iter #5600:   Batch Loss = 0.193933, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1924523413181305, Accuracy = 1.0\n",
      "Training iter #5700:   Batch Loss = 0.192693, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1912485659122467, Accuracy = 1.0\n",
      "Training iter #5800:   Batch Loss = 0.191467, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.19005805253982544, Accuracy = 1.0\n",
      "Training iter #5900:   Batch Loss = 0.190254, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.18888068199157715, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #6000:   Batch Loss = 0.189054, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1877160668373108, Accuracy = 1.0\n",
      "Training iter #6100:   Batch Loss = 0.187868, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.18656405806541443, Accuracy = 1.0\n",
      "Training iter #6200:   Batch Loss = 0.186694, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.18542444705963135, Accuracy = 1.0\n",
      "Training iter #6300:   Batch Loss = 0.185533, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1842970997095108, Accuracy = 1.0\n",
      "Training iter #6400:   Batch Loss = 0.184385, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.18318162858486176, Accuracy = 1.0\n",
      "Training iter #6500:   Batch Loss = 0.183248, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1820780634880066, Accuracy = 1.0\n",
      "Training iter #6600:   Batch Loss = 0.182123, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.18098598718643188, Accuracy = 1.0\n",
      "Training iter #6700:   Batch Loss = 0.181010, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1799052357673645, Accuracy = 1.0\n",
      "Training iter #6800:   Batch Loss = 0.179908, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.17883579432964325, Accuracy = 1.0\n",
      "Training iter #6900:   Batch Loss = 0.178817, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.17777717113494873, Accuracy = 1.0\n",
      "Training iter #7000:   Batch Loss = 0.177737, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.17672939598560333, Accuracy = 1.0\n",
      "Training iter #7100:   Batch Loss = 0.176667, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.17569223046302795, Accuracy = 1.0\n",
      "Training iter #7200:   Batch Loss = 0.175608, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.17466560006141663, Accuracy = 1.0\n",
      "Training iter #7300:   Batch Loss = 0.174560, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.17364907264709473, Accuracy = 1.0\n",
      "Training iter #7400:   Batch Loss = 0.173521, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.17264266312122345, Accuracy = 1.0\n",
      "Training iter #7500:   Batch Loss = 0.172492, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.17164622247219086, Accuracy = 1.0\n",
      "Training iter #7600:   Batch Loss = 0.171473, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1706594079732895, Accuracy = 1.0\n",
      "Training iter #7700:   Batch Loss = 0.170463, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.16968227922916412, Accuracy = 1.0\n",
      "Training iter #7800:   Batch Loss = 0.169462, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.16871437430381775, Accuracy = 1.0\n",
      "Training iter #7900:   Batch Loss = 0.168471, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1677558869123459, Accuracy = 1.0\n",
      "Training iter #8000:   Batch Loss = 0.167488, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.16680648922920227, Accuracy = 1.0\n",
      "Training iter #8100:   Batch Loss = 0.166514, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.16586588323116302, Accuracy = 1.0\n",
      "Training iter #8200:   Batch Loss = 0.165549, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.16493409872055054, Accuracy = 1.0\n",
      "Training iter #8300:   Batch Loss = 0.164592, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1640108972787857, Accuracy = 1.0\n",
      "Training iter #8400:   Batch Loss = 0.163644, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.16309607028961182, Accuracy = 1.0\n",
      "Training iter #8500:   Batch Loss = 0.162704, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.16218943893909454, Accuracy = 1.0\n",
      "Training iter #8600:   Batch Loss = 0.161773, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.16129079461097717, Accuracy = 1.0\n",
      "Training iter #8700:   Batch Loss = 0.160849, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.16040003299713135, Accuracy = 1.0\n",
      "Training iter #8800:   Batch Loss = 0.159933, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1595168113708496, Accuracy = 1.0\n",
      "Training iter #8900:   Batch Loss = 0.159026, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.15864107012748718, Accuracy = 1.0\n",
      "Training iter #9000:   Batch Loss = 0.158126, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.15777257084846497, Accuracy = 1.0\n",
      "Training iter #9100:   Batch Loss = 0.157234, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.15691088140010834, Accuracy = 1.0\n",
      "Training iter #9200:   Batch Loss = 0.156349, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.15605607628822327, Accuracy = 1.0\n",
      "Training iter #9300:   Batch Loss = 0.155472, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1552077680826187, Accuracy = 1.0\n",
      "Training iter #9400:   Batch Loss = 0.154603, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1543658822774887, Accuracy = 1.0\n",
      "Training iter #9500:   Batch Loss = 0.153741, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.15353012084960938, Accuracy = 1.0\n",
      "Training iter #9600:   Batch Loss = 0.152887, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.15270033478736877, Accuracy = 1.0\n",
      "Training iter #9700:   Batch Loss = 0.152040, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.15187640488147736, Accuracy = 1.0\n",
      "Training iter #9800:   Batch Loss = 0.151200, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.15105809271335602, Accuracy = 1.0\n",
      "Training iter #9900:   Batch Loss = 0.150368, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1502452790737152, Accuracy = 1.0\n",
      "Training iter #10000:   Batch Loss = 0.149542, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.14943794906139374, Accuracy = 1.0\n",
      "Training iter #10100:   Batch Loss = 0.148724, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.14863598346710205, Accuracy = 1.0\n",
      "Training iter #10200:   Batch Loss = 0.147912, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1478392332792282, Accuracy = 1.0\n",
      "Training iter #10300:   Batch Loss = 0.147108, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.14704766869544983, Accuracy = 1.0\n",
      "Training iter #10400:   Batch Loss = 0.146310, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.14626134932041168, Accuracy = 1.0\n",
      "Training iter #10500:   Batch Loss = 0.145519, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.145480215549469, Accuracy = 1.0\n",
      "Training iter #10600:   Batch Loss = 0.144734, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.14470425248146057, Accuracy = 1.0\n",
      "Training iter #10700:   Batch Loss = 0.143956, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.14393362402915955, Accuracy = 1.0\n",
      "Training iter #10800:   Batch Loss = 0.143184, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.14316821098327637, Accuracy = 1.0\n",
      "Training iter #10900:   Batch Loss = 0.142419, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.14240805804729462, Accuracy = 1.0\n",
      "Training iter #11000:   Batch Loss = 0.141660, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.14165322482585907, Accuracy = 1.0\n",
      "Training iter #11100:   Batch Loss = 0.140907, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.14090363681316376, Accuracy = 1.0\n",
      "Training iter #11200:   Batch Loss = 0.140160, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1401594877243042, Accuracy = 1.0\n",
      "Training iter #11300:   Batch Loss = 0.139418, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.139420747756958, Accuracy = 1.0\n",
      "Training iter #11400:   Batch Loss = 0.138683, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.13868729770183563, Accuracy = 1.0\n",
      "Training iter #11500:   Batch Loss = 0.137954, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.13795916736125946, Accuracy = 1.0\n",
      "Training iter #11600:   Batch Loss = 0.137230, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.13723646104335785, Accuracy = 1.0\n",
      "Training iter #11700:   Batch Loss = 0.136512, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.13651910424232483, Accuracy = 1.0\n",
      "Training iter #11800:   Batch Loss = 0.135799, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.135807067155838, Accuracy = 1.0\n",
      "Training iter #11900:   Batch Loss = 0.135092, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.1351003795862198, Accuracy = 1.0\n",
      "Training iter #12000:   Batch Loss = 0.134390, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.13439896702766418, Accuracy = 1.0\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULT: Batch Loss = 0.13439896702766418, Accuracy = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./gesture_model'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "step = 1\n",
    "while step * batch_size <= training_iters:\n",
    "    batch_xs = extract_batch_size(X_train, step, batch_size)\n",
    "    batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "    # check that encoded output is same length as num_classes, if not, pad it \n",
    "    if len(batch_ys[0]) < n_classes:\n",
    "        temp_ys = np.zeros((batch_size, n_classes))\n",
    "        temp_ys[:batch_ys.shape[0],:batch_ys.shape[1]] = batch_ys\n",
    "        batch_ys = temp_ys\n",
    "       \n",
    "    \n",
    "\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "              \"Batch Loss = {}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Accuracy for test data\n",
    "\n",
    "one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "    [pred, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: X_test,\n",
    "        y: one_hot(y_test)\n",
    "    }\n",
    ")\n",
    "\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "\n",
    "print(\"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss = {}\".format(final_loss) + \\\n",
    "      \", Accuracy = {}\".format(accuracy))\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, \"./gesture_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=[[1,2],[3,4]]\n",
    "x=np.array([x])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "camera = cv2.VideoCapture(0)\n",
    "camera.set(10,200)\n",
    "\n",
    "ret, frame = camera.read()\n",
    "while (1):\n",
    "   \n",
    "    ret, frame = camera.read()\n",
    "    frame = cv2.resize(frame, (480, 640)) \n",
    "    cv2.imshow('h',frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
